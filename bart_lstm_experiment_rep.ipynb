{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:33:25.683949Z",
     "start_time": "2024-12-20T08:33:25.504791Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Some Requirements on Load Config Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:50:21.153949Z",
     "start_time": "2024-12-20T08:50:21.130747Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import re\n",
    "from dataclasses import dataclass, field\n",
    "from multiprocessing import cpu_count\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Requirements for building Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:50:29.369889Z",
     "start_time": "2024-12-20T08:50:21.880365Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict, ClassLabel, load_from_disk\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.utilities.rank_zero import rank_zero_info\n",
    "\n",
    "#from config import Config, DataModuleConfig, ModuleConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Requirements for Denfinting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:50:29.541529Z",
     "start_time": "2024-12-20T08:50:29.373400Z"
    }
   },
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.utilities.types import OptimizerLRScheduler, EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "from lightning.pytorch.utilities.types import OptimizerLRScheduler\n",
    "\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchmetrics.classification import MultilabelAccuracy, MultilabelPrecision, MultilabelRecall, MultilabelF1Score\n",
    "import torchinfo\n",
    "\n",
    "from transformers import BertForSequenceClassification, AutoModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import CSVLogger, CometLogger, TensorBoardLogger\n",
    "from lightning.pytorch.profilers import PyTorchProfiler\n",
    "\n",
    "from dvclive.lightning import DVCLiveLogger\n",
    "\n",
    "from datamodule import AutoTokenizerDataModule\n",
    "from module import CustomModel\n",
    "from utils import create_dirs\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "token = os.getenv('HUG_FACE_TOKEN')\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:50:29.572536Z",
     "start_time": "2024-12-20T08:50:29.559534Z"
    }
   },
   "outputs": [],
   "source": [
    "this_kaggle = \"./\"\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    cache_dir: str = os.path.join(this_kaggle, \"data\")\n",
    "    log_dir: str = os.path.join(this_kaggle, \"logs\")\n",
    "    ckpt_dir: str = os.path.join(this_kaggle, \"checkpoints\")\n",
    "    prof_dir: str = os.path.join(this_kaggle, \"logs\", \"profiler\")\n",
    "    perf_dir: str = os.path.join(this_kaggle, \"logs\", \"perf\")\n",
    "    seed: int = 59631546\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModuleConfig:\n",
    "    model_name: str = \"facebook/bart-large-mnli\" # change this to use a different pretrained checkpoint and tokenizer\n",
    "    # model_name: str = \"nvidia/NV-Embed-v2\" # change this to use a different pretrained checkpoint and tokenizer\n",
    "    learning_rate: float = 1.0103374612260327e-5\n",
    "    learning_rate_bert: float = 1.0103374612260327e-5\n",
    "    learning_rate_lstm: float = 7e-5\n",
    "    finetuned: str = \"checkpoints/twhin-bert-base-finetuned\" # change this to use a different pretrained checkpoint and tokenizer\n",
    "    max_length: int = 128\n",
    "    attention_probs_dropout: float = 0.1\n",
    "    classifier_dropout: Optional[float] = None\n",
    "    warming_steps: int = 100\n",
    "    focal_gamma: float = 2.0\n",
    "\n",
    "    #opposing_label_sets: List[Tuple[int, int]] = field(default_factory=lambda: [(0, 1), (10, 11)])\n",
    "#ModuleConfig.opposing_label_sets = [(0, 1), (10, 11)]\n",
    "ModuleConfig.opposing_label_sets = None\n",
    "@dataclass\n",
    "class DataModuleConfig:\n",
    "    dataset_name: str = \"sdy623/new_disaster_tweets\" # change this to use different dataset\n",
    "    num_classes: int = 12\n",
    "    batch_size: int = 8\n",
    "    train_split: str = \"train\"\n",
    "    test_split: str = \"test\"\n",
    "    train_size: float = 0.8\n",
    "    stratify_by_column: str = \"label\"\n",
    "    num_workers: int = 0\n",
    "\n",
    "@dataclass\n",
    "class TrainerConfig:\n",
    "    accelerator: str = \"auto\" # Trainer flag\n",
    "    devices: Union[int, str] = \"auto\"  # Trainer flag\n",
    "    strategy: str = \"auto\"  # Trainer flag\n",
    "    precision: Optional[str] = \"bf16\"  # Trainer flag\n",
    "    max_epochs: int = 7  # Trainer flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and dataset\n",
    "model_name = ModuleConfig.model_name\n",
    "max_length = ModuleConfig.max_length\n",
    "lr = ModuleConfig.learning_rate\n",
    "dataset_name = DataModuleConfig.dataset_name\n",
    "batch_size = DataModuleConfig.batch_size\n",
    "\n",
    "# paths\n",
    "cache_dir = Config.cache_dir\n",
    "log_dir = Config.log_dir\n",
    "ckpt_dir = Config.ckpt_dir\n",
    "prof_dir = Config.prof_dir\n",
    "perf_dir = Config.perf_dir\n",
    "# creates dirs to avoid failure if empty dir has been deleted\n",
    "create_dirs([cache_dir, log_dir, ckpt_dir, prof_dir, perf_dir])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:50:29.603774Z",
     "start_time": "2024-12-20T08:50:29.589775Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_text(\n",
    "    batch: dict,\n",
    "    *,\n",
    "    model_name: str,\n",
    "    cache_dir: Union[str, Path],\n",
    "    truncation: bool = True,  # leave as True if dataset has sequences that exceed the model's max sequence length\n",
    "    padding: bool = \"max_length\",  # pad so that all tensors are of the same dimensions\n",
    "    max_length: int = 512,\n",
    "):\n",
    "    \"\"\"\n",
    "    Notes:\n",
    "        https://huggingface.co/docs/transformers/v4.38.2/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained\n",
    "    \"\"\"\n",
    "    #print(batch[\"text\"])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    text = batch if isinstance(batch, str) else batch[\"text\"]  # allow for inference input as raw text\n",
    "    if isinstance(text, list):\n",
    "        text = [str(t) for t in text]\n",
    "    else:\n",
    "        text = str(text)\n",
    "    final_token = tokenizer(text, truncation=truncation, padding=padding, return_tensors=\"pt\", return_token_type_ids=True, max_length=max_length)\n",
    "    #print(tokenizer.model_max_length)\n",
    "    '''\n",
    "    for i in range(len(final_token['input_ids'])):\n",
    "                assert len(final_token['input_ids'][i]) == max_length, f\"input_ids length mismatch at index {i}, expected {max_length} but got {len(final_token['input_ids'][i])}\"\n",
    "                assert len(final_token['attention_mask'][i]) == max_length, f\"attention_mask length mismatch at index {i}, expected {max_length} but got {len(final_token['attention_mask'][i])}\"\n",
    "                if 'token_type_ids' in final_token:\n",
    "                    assert len(final_token['token_type_ids'][i]) == max_length, f\"token_type_ids length mismatch at index {i}, expected {max_length} but got {len(final_token['token_type_ids'][i])}\"\n",
    "    #assert len(segment_ids) == max_seq_length\n",
    "    '''\n",
    "    \n",
    "    return final_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare The Data Processing Class and Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return max(lr_factor, 2e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:51:24.810176Z",
     "start_time": "2024-12-20T08:51:24.790173Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, default_collate\n",
    "\n",
    "class AutoTokenizerDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name: str = DataModuleConfig.dataset_name,\n",
    "        cache_dir: Union[str, Path] = Config.cache_dir,\n",
    "        model_name: str = ModuleConfig.model_name,\n",
    "        max_length: int = ModuleConfig.max_length,\n",
    "        num_labels: int = DataModuleConfig.num_classes,\n",
    "        columns: list = [\"input_ids\", \"attention_mask\", \"label\", \"token_type_ids\"],\n",
    "        batch_size: int = DataModuleConfig.batch_size,\n",
    "        train_size: float = DataModuleConfig.train_size,\n",
    "        stratify_by_column: str = DataModuleConfig.stratify_by_column,\n",
    "        train_split: str = DataModuleConfig.train_split,\n",
    "        test_split: str = DataModuleConfig.test_split,\n",
    "        num_workers: int = DataModuleConfig.num_workers,\n",
    "        seed: int = Config.seed,\n",
    "    ) -> None:\n",
    "        \"\"\"a custom PyTorch Lightning LightningDataModule to tokenize text datasets\n",
    "\n",
    "        Args:\n",
    "            dataset_name: the name of the dataset as given on HF datasets\n",
    "            cache_dir: corresponds to HF datasets.load_dataset\n",
    "            model_name: the name of the model and accompanying tokenizer\n",
    "            num_labels: the number of labels\n",
    "            columns: the list of column names to pass to the HF dataset's .set_format method\n",
    "            batch_size: the batch size to pass to the PyTorch DataLoaders\n",
    "            train_size: the size of the training data split to pass to .train_test_split\n",
    "            stratify_by_column: column name of labels to be used to perform stratified split of data\n",
    "            train_split: the name of the training split as given on HF Hub\n",
    "            test_split: the name of the test split as given on HF Hub\n",
    "            num_workers: corresponds to torch.utils.data.DataLoader\n",
    "            seed: the seed used in lightning.pytorch.seed_everything\n",
    "\n",
    "        Notes:\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dataset_name = dataset_name\n",
    "        self.cache_dir = cache_dir\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.train_size = train_size\n",
    "        self.train_split = train_split\n",
    "        self.test_split = test_split\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.seed = seed\n",
    "        self.num_labels = num_labels\n",
    "        self.columns = columns\n",
    "        self.stratify_by_column = stratify_by_column\n",
    "\n",
    "    def clear_custom_cache(self):\n",
    "        \"\"\"Custom method to clear cache\"\"\"\n",
    "        if os.path.exists(self.cache_dir):\n",
    "            shutil.rmtree(self.cache_dir)  # Remove the directory\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Notes:\n",
    "            https://lightning.ai/docs/pytorch/stable/data/datamodule.html#prepare-data\n",
    "        \"\"\"\n",
    "        pl.seed_everything(self.seed)\n",
    "        # disable parrelism to avoid deadlocks\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "        if not os.path.isdir(self.cache_dir):\n",
    "            os.mkdir(self.cache_dir)\n",
    "\n",
    "        cache_dir_is_empty = len(os.listdir(self.cache_dir)) == 0\n",
    "\n",
    "        if cache_dir_is_empty:\n",
    "            rank_zero_info(f\"[{str(datetime.now())}] Downloading dataset.\")\n",
    "            load_dataset(self.dataset_name, cache_dir=self.cache_dir, use_auth_token=True)\n",
    "        else:\n",
    "            rank_zero_info(\n",
    "                f\"[{str(datetime.now())}] Data cache exists. Loading from cache.\"\n",
    "            )\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        \"\"\"\n",
    "        Notes:\n",
    "            https://lightning.ai/docs/pytorch/stable/data/datamodule.html#setup\n",
    "        \"\"\"\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            # load and split\n",
    "            dataset = load_dataset(\n",
    "                self.dataset_name, cache_dir=self.cache_dir\n",
    "            )\n",
    "            print(dataset)\n",
    "            #dataset = dataset.train_test_split(\n",
    "            #    train_size=self.train_size, stratify_by_column=self.stratify_by_column\n",
    "            #)\n",
    "            # prep train\n",
    "            self.train_data = dataset[\"train\"].map(\n",
    "                lambda example: {'text': example['text'], \n",
    "                                'label': list(example.values())[2:]},\n",
    "                batched=False\n",
    "            )            \n",
    "            self.train_data = self.train_data.map(\n",
    "                tokenize_text,\n",
    "                batched=True,\n",
    "                batch_size=1024,\n",
    "                fn_kwargs={\"model_name\": self.model_name, \"cache_dir\": self.cache_dir, \"max_length\": self.max_length},\n",
    "            )\n",
    "            \n",
    "            self.train_data.set_format(\"torch\", columns=self.columns, output_all_columns=True)            \n",
    "            # prep val\n",
    "            self.val_data = dataset[\"test\"].map(\n",
    "                lambda example: {'text': example['text'], \n",
    "                                'label': list(example.values())[2:]},\n",
    "                batched=False\n",
    "            )\n",
    "            \n",
    "            self.val_data = self.val_data.map(\n",
    "                tokenize_text,\n",
    "                batched=True,\n",
    "                batch_size=1024,\n",
    "                fn_kwargs={\"model_name\": self.model_name, \"cache_dir\": self.cache_dir, \"max_length\": self.max_length},\n",
    "            )\n",
    "\n",
    "            self.val_data.set_format(\"torch\", columns=self.columns)            \n",
    "            # free mem from unneeded dataset obj\n",
    "            del dataset\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_data = load_dataset(\n",
    "                self.dataset_name, split=self.test_split, cache_dir=self.cache_dir\n",
    "            )\n",
    "            self.test_data.map(\n",
    "                tokenize_text,\n",
    "                batched=True,\n",
    "                batch_size=512,\n",
    "                fn_kwargs={\"model_name\": self.model_name, \"cache_dir\": self.cache_dir, \"max_length\": self.max_length},\n",
    "            )\n",
    "            self.test_data.set_format(\"torch\", columns=self.columns)\n",
    "\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        \"\"\"\n",
    "        Notes:\n",
    "            https://lightning.ai/docs/pytorch/stable/data/datamodule.html#train-dataloader\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.train_data,\n",
    "            num_workers=self.num_workers,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=lambda batch: default_collate([item for item in batch if item is not None])\n",
    "\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        \"\"\"\n",
    "        Notes:\n",
    "            https://lightning.ai/docs/pytorch/stable/data/datamodule.html#val-dataloader\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.val_data,\n",
    "            num_workers=self.num_workers,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        \"\"\"\n",
    "        Notes:\n",
    "            https://lightning.ai/docs/pytorch/stable/data/datamodule.html#test-dataloader\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.test_data,\n",
    "            num_workers=self.num_workers,\n",
    "            batch_size=self.batch_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Defination of Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:52:35.504030Z",
     "start_time": "2024-12-20T08:52:35.478001Z"
    }
   },
   "outputs": [],
   "source": [
    "@abstractmethod\n",
    "class EncoderBase(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.encoder.config.hidden_size\n",
    "\n",
    "\n",
    "class BARTEmbeddings(EncoderBase):\n",
    "    def __init__(self, model_name,\n",
    "                 attention_dropout: Optional[float] = None):\n",
    "        super().__init__(model_name)\n",
    "        self.hidden_size = self.encoder.config.hidden_size\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "\n",
    "        outputs = self.encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            decoder_head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "class BERTEmbeeding(EncoderBase):\n",
    "    def __init__(self, model_name,\n",
    "                 attention_probs_dropout: Optional[float] = None):\n",
    "        super().__init__(model_name)\n",
    "        assert attention_probs_dropout is None or 0 <= attention_probs_dropout <= 1, \\\n",
    "            \"attention_probs_dropout must be between 0 and 1 or None\"\n",
    "        self.hidden_size = self.encoder.config.hidden_size\n",
    "        if attention_probs_dropout:\n",
    "            self.encoder = AutoModel.from_pretrained(\n",
    "                model_name,\n",
    "                attention_probs_dropout_prob=attention_probs_dropout\n",
    "            )\n",
    "    def forward(self, input_ids, \n",
    "                attention_mask=None, \n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                head_mask=None,):\n",
    "        \n",
    "        outputs = self.encoder(input_ids=input_ids,\n",
    "                               attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids,\n",
    "                               position_ids=position_ids,\n",
    "                               head_mask=head_mask,)\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "class ClassifierBase(nn.Module):\n",
    "    def __init__(self, input_dim, num_labels):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        raise NotImplementedError(\"This method should be overridden in subclasses\")\n",
    "\n",
    "class BertLinearClassificationHead(ClassifierBase):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim: int,\n",
    "            num_labels: int,\n",
    "            opposing_label_sets: List[Tuple[int, int]] = None,\n",
    "            classifier_dropout: Optional[float] = None\n",
    "    ):\n",
    "        assert classifier_dropout is None or 0 <= classifier_dropout <= 1, \\\n",
    "            \"pooler_dropout must be between 0 and 1 or None\"\n",
    "        super().__init__(input_dim, num_labels)\n",
    "        self.opposing_label_sets = opposing_label_sets  # List of tuples with opposing label indices\n",
    "        self.dropout = nn.Dropout(p=classifier_dropout) if classifier_dropout else None\n",
    "        self.linear = nn.Linear(input_dim, num_labels)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        #pooled_output = torch.mean(hidden_states, dim=1)\n",
    "        cls_output = hidden_states[:, 0, :]\n",
    "        if self.dropout:\n",
    "            cls_output = self.dropout(cls_output)\n",
    "        logits = self.linear(cls_output)\n",
    "\n",
    "        # Apply Softmax to opposing labels\n",
    "        if self.opposing_label_sets is not None:\n",
    "            for label_set in self.opposing_label_sets:\n",
    "                logits[:, label_set] = torch.softmax(logits[:, label_set], dim=1).to(logits.dtype)\n",
    "\n",
    "        # Apply Sigmoid to all logits for multi-label outputs\n",
    "        return logits\n",
    "\n",
    "class ClassificationHEAD(ClassifierBase):\n",
    "    def __init__(self, input_dim, num_labels, opposing_label_sets: List[Tuple[int, int]]=None):\n",
    "        super().__init__(input_dim, num_labels)\n",
    "        self.opposing_label_sets = opposing_label_sets  # List of tuples with opposing label indices\n",
    "        self.linear = nn.Linear(input_dim, num_labels)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        #pooled_output = torch.mean(hidden_states, dim=1)\n",
    "        cls_output = hidden_states[:, 0, :]\n",
    "        logits = self.linear(cls_output)\n",
    "\n",
    "        # Apply Softmax to opposing labels\n",
    "        if self.opposing_label_sets is not None:\n",
    "            for label_set in self.opposing_label_sets:\n",
    "                logits[:, label_set] = torch.softmax(logits[:, label_set], dim=1)\n",
    "        \n",
    "        # Apply Sigmoid to all logits for multi-label outputs\n",
    "        return logits\n",
    "\n",
    "class LSTMClassificationHEAD(ClassifierBase):\n",
    "    def __init__(self, input_dim, num_labels, opposing_label_sets: List[Tuple[int, int]]=None):\n",
    "        super().__init__(input_dim, num_labels)\n",
    "        self.opposing_label_sets = opposing_label_sets  # List of tuples with opposing label indices\n",
    "        self.lstm = nn.LSTM(input_dim, input_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(input_dim, num_labels)\n",
    "        #self.multiAttn = nn.MultiheadAttention(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, last_hidden_state):\n",
    "        out, _ = self.lstm(last_hidden_state, None)\n",
    "        \n",
    "        sequence_output = out[:, -1, :]\n",
    "        logits = self.linear(sequence_output)\n",
    "        \n",
    "        # Apply Softmax to opposing labels\n",
    "        '''\n",
    "        if self.opposing_label_sets is not None:\n",
    "            for label_set in self.opposing_label_sets:\n",
    "                logits[:, label_set] = torch.softmax(logits[:, label_set], dim=1)\n",
    "        '''\n",
    "        # Apply Sigmoid to all logits for multi-label outputs\n",
    "        return logits\n",
    "\n",
    "class CustomModel(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "        model_name: str = ModuleConfig.model_name,\n",
    "        num_classes: int = DataModuleConfig.num_classes,  # set according to the finetuning dataset\n",
    "        input_key: str = \"input_ids\",  # set according to the finetuning dataset\n",
    "        label_key: str = \"label\",  # set according to the finetuning dataset\n",
    "        mask_key: str = \"attention_mask\",  # set according to the model output object\n",
    "        output_key: str = \"logits\",  # set according to the model output object\n",
    "        loss_key: str = \"loss\",  # set according to the model output object\n",
    "        attention_probs_dropout: Optional[float] = ModuleConfig.attention_probs_dropout,\n",
    "        classifier_dropout: Optional[float] = ModuleConfig.classifier_dropout,\n",
    "        learning_rate: float = ModuleConfig.learning_rate,\n",
    "        learning_rate_bert: float = ModuleConfig.learning_rate_bert,\n",
    "        learning_rate_lstm: float = ModuleConfig.learning_rate_lstm,\n",
    "        lr_gamma: float = 0.76825,\n",
    "        opposing_label_sets: List[Tuple[int, int]] = None,\n",
    "        warmup: int = ModuleConfig.warming_steps,):\n",
    "\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.input_key = input_key\n",
    "        self.label_key = label_key\n",
    "        self.mask_key = mask_key\n",
    "        self.output_key = output_key\n",
    "        self.loss_key = loss_key\n",
    "        self.num_classes = num_classes\n",
    "        self.lr_gamma= lr_gamma\n",
    "        \n",
    "        self.encoder = BARTEmbeddings(model_name, attention_probs_dropout)\n",
    "\n",
    "        self.classifier = LSTMClassificationHEAD(self.encoder.hidden_size, num_classes, opposing_label_sets)\n",
    "        #for name, param in self.encoder.encoder.named_parameters():\n",
    "        #    print(name)\n",
    "        #for param in self.encoder.encoder.parameters():\n",
    "        #    param.requires_grad = False\n",
    "        #    # Print all layers of BERT\n",
    "        # Unfreeze the last four layers in BERT\n",
    "        #for layer in self.encoder.encoder.encoder.layer[-4:]:\n",
    "        #    for param in layer.parameters():\n",
    "        #        param.requires_grad = True\n",
    "                \n",
    "        self.learning_rate = learning_rate\n",
    "        self.learning_rate_bert = learning_rate_bert\n",
    "        self.learning_rate_lstm = learning_rate_lstm\n",
    "\n",
    "        self.opposing_label_sets = opposing_label_sets\n",
    "        self.criterion = nn.BCEWithLogitsLoss()  # Use BCEWithLogits for multi-label setting\n",
    "        #self.criterion = FocalLoss(gamma=ModuleConfig.focal_gamma, reduction='sum')\n",
    "        # Metrics\n",
    "        self.accuracy = MultilabelAccuracy(num_labels=num_classes, average='micro')\n",
    "        self.precision = MultilabelPrecision(num_labels=num_classes, average='micro')\n",
    "        self.recall = MultilabelRecall(num_labels=num_classes, average='micro')\n",
    "        self.f1_score = MultilabelF1Score(num_labels=num_classes, average='micro')\n",
    "        self.macro_f1_score = MultilabelF1Score(num_labels=num_classes, average='macro')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        #print(input_ids)\n",
    "        #print(attention_mask)\n",
    "        #print(token_type_ids)\n",
    "        hidden_states = self.encoder(input_ids, attention_mask)\n",
    "        logits = self.classifier(hidden_states)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #print(batch)\n",
    "        #input_ids, attention_mask, token_type_ids, labels = batch\n",
    "        #print(input_ids)\n",
    "        #print(attention_mask)\n",
    "        #print(token_type_ids)\n",
    "        #print(labels)\n",
    "        # logits = self(input_ids=batch[self.input_key], attention_mask=batch[self.mask_key], token_type_ids=batch[\"token_type_ids\"])\n",
    "        logits = self(input_ids=batch[self.input_key], attention_mask=batch[self.mask_key])\n",
    "        loss = self.criterion(logits, batch[self.label_key].float())\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits = self(input_ids=batch[self.input_key], attention_mask=batch[self.mask_key])\n",
    "        loss = self.criterion(logits, batch[self.label_key].float())\n",
    "        # preds = (torch.sigmoid(logits) > 0.5).int()\n",
    "        \n",
    "        all_opposing_labels = []\n",
    "        if self.opposing_label_sets is not None:\n",
    "            for label_set in self.opposing_label_sets:\n",
    "                all_opposing_labels.extend(label_set)\n",
    "                logits[:, label_set] = torch.softmax(logits[:, label_set].float(), dim=1).to(logits.dtype)\n",
    "\n",
    "        non_opposing_labels = [i for i in range(self.num_classes) if i not in all_opposing_labels]\n",
    "        if non_opposing_labels:\n",
    "            logits[:, non_opposing_labels] = torch.sigmoid(logits[:, non_opposing_labels])\n",
    "        \n",
    "        preds = (logits > 0.5).int()\n",
    "\n",
    "        # Calculate metrics\n",
    "        acc = self.accuracy(preds, batch[self.label_key])\n",
    "        prec = self.precision(preds, batch[self.label_key])\n",
    "        rec = self.recall(preds, batch[self.label_key])\n",
    "        f1 = self.f1_score(preds, batch[self.label_key])\n",
    "        marco_f1 = self.macro_f1_score(preds, batch[self.label_key])\n",
    "\n",
    "        # Log metrics\n",
    "        self.log(\"val_loss\", loss, on_epoch=True)\n",
    "        self.log(\"val_accuracy\", acc, on_epoch=True)\n",
    "        self.log(\"val_precision\", prec, on_epoch=True)\n",
    "        self.log(\"val_recall\", rec, on_epoch=True)\n",
    "        self.log(\"val_f1\", f1, on_epoch=True)\n",
    "        self.log(\"val_macro_f1\", marco_f1, on_epoch=True)\n",
    "\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": acc, \"val_precision\": prec, \"val_recall\": rec, \"val_f1\": f1, \"val_macro_f1\": marco_f1,}\n",
    "    \n",
    "    \n",
    "    def predict_step(\n",
    "        self, sequence: str, threshold: float = 0.5, cache_dir: Union[str, Path] = Config.cache_dir\n",
    "        ) -> str:\n",
    "            \"\"\"\n",
    "            Notes:\n",
    "                https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#inference\n",
    "            \"\"\"\n",
    "            batch = tokenize_text(sequence, model_name=self.model_name, cache_dir=cache_dir)\n",
    "            # autotokenizer may cause tokens to lose device type and cause failure\n",
    "            batch = batch.to(self.device)\n",
    "            outputs = self.model(batch[self.input_key])\n",
    "            predicted_labels = (outputs >= threshold).int()\n",
    "            labels = {\n",
    "                0: \"class_a\",\n",
    "                1: \"class_b\",\n",
    "                2: \"class_c\",\n",
    "                3: \"class_d\",\n",
    "                4: \"class_e\",\n",
    "                5: \"class_f\"\n",
    "            }\n",
    "            \n",
    "            # Return the label corresponding to the predicted index\n",
    "            return labels.get(predicted_labels, \"Unknown\")\n",
    "            #logits = outputs[self.output_key]\n",
    "            #predicted_label_id = torch.argmax(logits)\n",
    "            #labels = {0: \"negative\", 1: \"positive\"}\n",
    "            return labels[predicted_label_id.item()]\n",
    "\n",
    "    def test_step(self, batch, batch_idx) -> None:\n",
    "        \"\"\"\n",
    "        Notes:\n",
    "            https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#testing\n",
    "        \"\"\"\n",
    "        outputs = self(\n",
    "            batch[self.input_key],\n",
    "            attention_mask=batch[self.mask_key],\n",
    "        )\n",
    "\n",
    "        logits = outputs\n",
    "        predicted_labels = torch.argmax(logits, 1)\n",
    "        return predicted_labels.cpu().numpy()\n",
    "        \n",
    "    def configure_optimizers(self) -> OptimizerLRScheduler:\n",
    "        \n",
    "        bert_params = [param for name, param in self.named_parameters() if \"bert\" in name]\n",
    "        lstm_params = [param for name, param in self.named_parameters() if \"lstm\" in name or \"linear\" in name]\n",
    "        \n",
    "        '''\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {\"params\": bert_params, \"lr\": self.learning_rate_bert},\n",
    "            {\"params\": lstm_params, \"lr\": self.learning_rate_lstm}\n",
    "        ])\n",
    "        '''\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        #optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate, momentum=0.92, weight_decay=1.28e-5)\n",
    "        #print(total_steps)\n",
    "        self.lr_scheduler = CosineWarmupScheduler(\n",
    "            optimizer, warmup=self.hparams.warmup, max_iters=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        \n",
    "        fixed_lr_scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1, total_iters=3)\n",
    "        # Define the cosine annealing learning rate scheduler for the remaining epochs\n",
    "        cosine_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=5e-7)\n",
    "\n",
    "        reduce_lr_on_plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.35, patience=3, verbose=True)\n",
    "        # Create the exp scheduler\n",
    "        #exp_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.59825)\n",
    "        exp_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=self.lr_gamma)\n",
    "        # Combine the schedulers\n",
    "        combined_scheduler = torch.optim.lr_scheduler.ChainedScheduler(schedulers=[fixed_lr_scheduler, exp_scheduler])\n",
    "\n",
    "        return optimizer\n",
    "    \n",
    "    def optimizer_step(self, *args, **kwargs):\n",
    "        super().optimizer_step(*args, **kwargs)\n",
    "        self.lr_scheduler.step()  # Step per iteration\n",
    "        '''\n",
    "    def configure_optimizers(self) -> OptimizerLRScheduler:\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:52:36.338506Z",
     "start_time": "2024-12-20T08:52:36.325504Z"
    }
   },
   "outputs": [],
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:52:36.755055Z",
     "start_time": "2024-12-20T08:52:36.747053Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = ModuleConfig.model_name\n",
    "lr = ModuleConfig.learning_rate\n",
    "dataset_name = DataModuleConfig.dataset_name\n",
    "batch_size = DataModuleConfig.batch_size\n",
    "\n",
    "# paths\n",
    "cache_dir = Config.cache_dir\n",
    "log_dir = Config.log_dir\n",
    "ckpt_dir = Config.ckpt_dir\n",
    "prof_dir = Config.prof_dir\n",
    "perf_dir = Config.perf_dir\n",
    "# creates dirs to avoid failure if empty dir has been deleted\n",
    "create_dirs([cache_dir, log_dir, ckpt_dir, prof_dir, perf_dir])\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:52:37.328853Z",
     "start_time": "2024-12-20T08:52:37.318346Z"
    }
   },
   "outputs": [],
   "source": [
    "lit_datamodule = AutoTokenizerDataModule(\n",
    "    model_name=model_name,\n",
    "    dataset_name=dataset_name,\n",
    "    cache_dir=cache_dir,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:52:39.200793Z",
     "start_time": "2024-12-20T08:52:39.186688Z"
    }
   },
   "outputs": [],
   "source": [
    "lit_datamodule.clear_custom_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:52:40.022731Z",
     "start_time": "2024-12-20T08:52:39.966307Z"
    }
   },
   "outputs": [],
   "source": [
    "lit_datamodule.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:52:41.832285Z",
     "start_time": "2024-12-20T08:52:41.794263Z"
    }
   },
   "outputs": [],
   "source": [
    "lit_datamodule.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:52:43.940028Z",
     "start_time": "2024-12-20T08:52:43.898778Z"
    }
   },
   "outputs": [],
   "source": [
    "#lit_datamodule.setup(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:39:20.175015Z",
     "start_time": "2024-12-20T08:39:18.688064Z"
    }
   },
   "outputs": [],
   "source": [
    "lit_model = CustomModel(learning_rate=lr, learning_rate_bert=lr, \n",
    "                    lr_gamma=0.75, attention_probs_dropout=0.1, \n",
    "                    classifier_dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:39:20.191017Z",
     "start_time": "2024-12-20T08:39:20.180017Z"
    }
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=ckpt_dir,\n",
    "        monitor=\"val_f1\",\n",
    "        filename=\"model\",\n",
    "        save_top_k=3,\n",
    "        mode=\"max\",\n",
    "        save_weights_only=True,\n",
    "    ),\n",
    "    LearningRateMonitor(logging_interval='step'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:39:20.221789Z",
     "start_time": "2024-12-20T08:39:20.206902Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = CSVLogger(\n",
    "    save_dir=log_dir,\n",
    "    name=\"csv-logs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:39:20.673242Z",
     "start_time": "2024-12-20T08:39:20.462576Z"
    }
   },
   "outputs": [],
   "source": [
    "lit_trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    strategy=\"auto\",\n",
    "    precision=\"bf16-mixed\",\n",
    "    max_epochs=8,\n",
    "    deterministic=True,\n",
    "    logger=[logger, CometLogger(api_key=\"YOUR_COMET_API_KEY\"), DVCLiveLogger(save_dvc_exp=True)],\n",
    "    callbacks=callbacks,\n",
    "    log_every_n_steps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-10T07:15:48.640324Z"
    }
   },
   "outputs": [],
   "source": [
    "lit_trainer.fit(model=lit_model, datamodule=lit_datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:30:23.170146Z",
     "start_time": "2024-12-20T08:30:21.046681Z"
    }
   },
   "outputs": [],
   "source": [
    "!bash -c cat {lit_trainer.log_dir}/metrics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {lit_trainer.log_dir}/metrics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -all checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model for inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:39:25.395807Z",
     "start_time": "2024-12-20T08:39:25.388810Z"
    }
   },
   "outputs": [],
   "source": [
    "lit_trainer.checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:40:32.239232Z",
     "start_time": "2024-12-20T08:40:27.828890Z"
    }
   },
   "outputs": [],
   "source": [
    "model = lit_model.load_from_checkpoint(\"./checkpoints/best.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:40:33.070402Z",
     "start_time": "2024-12-20T08:40:33.062404Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:40:33.704457Z",
     "start_time": "2024-12-20T08:40:33.686253Z"
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:40:35.331429Z",
     "start_time": "2024-12-20T08:40:35.287645Z"
    }
   },
   "outputs": [],
   "source": [
    "torchinfo.summary(model, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:40:36.044448Z",
     "start_time": "2024-12-20T08:40:36.024450Z"
    }
   },
   "outputs": [],
   "source": [
    "model['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:40:36.939681Z",
     "start_time": "2024-12-20T08:40:36.931683Z"
    }
   },
   "outputs": [],
   "source": [
    "model.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:40:37.757360Z",
     "start_time": "2024-12-20T08:40:37.590019Z"
    }
   },
   "outputs": [],
   "source": [
    "model.get('callbacks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:40:39.831625Z",
     "start_time": "2024-12-20T08:40:39.655187Z"
    }
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:40:41.342507Z",
     "start_time": "2024-12-20T08:40:40.335631Z"
    }
   },
   "outputs": [],
   "source": [
    "torch_org_model = torch.load(\"./checkpoints/best.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T08:46:22.341285Z",
     "start_time": "2024-12-20T08:46:22.300419Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare test_dataloader\n",
    "test_dataloader = lit_datamodule.test_dataloader()\n",
    "\n",
    "# Eval mode\n",
    "model.eval()\n",
    "\n",
    "# Prepare list for storing inf results\n",
    "predictions = []\n",
    "\n",
    "# Disable grad for inf\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        #print(batch)\n",
    "        input_ids = batch[model.input_key].to(model.device)\n",
    "        attention_mask = batch[model.mask_key].to(model.device)\n",
    "        #token_type_ids = batch[\"token_type_ids\"]\n",
    "        \n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        #print(torch.sigmoid(logits[:, 1]))\n",
    "        preds = (torch.sigmoid(logits)[:, 1]> 0.5).int()\n",
    "\n",
    "        predictions.extend(preds.cpu().numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 869809,
     "sourceId": 17777,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tw-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
