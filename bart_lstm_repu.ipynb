{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import re\n",
    "from dataclasses import dataclass, field\n",
    "from multiprocessing import cpu_count\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict, ClassLabel, load_from_disk\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.utilities.rank_zero import rank_zero_info\n",
    "\n",
    "#from config import Config, DataModuleConfig, ModuleConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.utilities.types import OptimizerLRScheduler, EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "from lightning.pytorch.utilities.types import OptimizerLRScheduler\n",
    "\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchmetrics.classification import MultilabelAccuracy, MultilabelPrecision, MultilabelRecall, MultilabelF1Score\n",
    "import torchinfo\n",
    "\n",
    "from transformers import BertForSequenceClassification, AutoModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import CSVLogger, CometLogger, TensorBoardLogger\n",
    "from lightning.pytorch.profilers import PyTorchProfiler\n",
    "\n",
    "from dvclive.lightning import DVCLiveLogger\n",
    "\n",
    "from datamodule import AutoTokenizerDataModule\n",
    "from module import CustomModel\n",
    "from utils import create_dirs\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "token = os.getenv('HUG_FACE_TOKEN')\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from multiprocessing import cpu_count\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, List, Tuple\n",
    "\n",
    "# ## get root path ## #\n",
    "'''\n",
    "this_file = Path(__file__)\n",
    "this_studio_idx = [\n",
    "    i for i, j in enumerate(this_file.parents) if j.name.endswith(\"this_studio\")\n",
    "][0]\n",
    "'''\n",
    "this_studio = \"./\"\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    cache_dir: str = os.path.join(this_studio, \"data\")\n",
    "    log_dir: str = os.path.join(this_studio, \"logs\")\n",
    "    ckpt_dir: str = os.path.join(r\"E:\\bert-twetter-disaster-model-trained\", \"checkpoints\")\n",
    "    prof_dir: str = os.path.join(this_studio, \"logs\", \"profiler\")\n",
    "    perf_dir: str = os.path.join(this_studio, \"logs\", \"perf\")\n",
    "    seed: int = 59631546\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModuleConfig:\n",
    "    model_name: str = \"facebook/bart-large-mnli\" # change this to use a different pretrained checkpoint and tokenizer\n",
    "    # model_name: str = \"nvidia/NV-Embed-v2\" # change this to use a different pretrained checkpoint and tokenizer\n",
    "    learning_rate: float = 1.0103374612260327e-5\n",
    "    learning_rate_bert: float = 1.0103374612260327e-5\n",
    "    learning_rate_lstm: float = 7e-5\n",
    "    finetuned: str = \"checkpoints/twhin-bert-base-finetuned\" # change this to use a different pretrained checkpoint and tokenizer\n",
    "    max_length: int = 128\n",
    "    attention_probs_dropout: float = 0.1\n",
    "    classifier_dropout: Optional[float] = None\n",
    "    warming_steps: int = 100\n",
    "    focal_gamma: float = 2.0\n",
    "    \n",
    "#ModuleConfig.opposing_label_sets = [(0, 1), (10, 11)]\n",
    "ModuleConfig.opposing_label_sets = None    \n",
    "@dataclass\n",
    "class DataModuleConfig:\n",
    "    dataset_name: str = \"sdy623/new_disaster_tweets\" # change this to use different dataset\n",
    "    num_classes: int = 12\n",
    "    batch_size: int = 8\n",
    "    train_split: str = \"train\"\n",
    "    test_split: str = \"test\"\n",
    "    train_size: float = 0.8\n",
    "    stratify_by_column: str = \"label\"\n",
    "    num_workers: int = 0\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class TrainerConfig:\n",
    "    accelerator: str = \"auto\" # Trainer flag\n",
    "    devices: Union[int, str] = \"auto\"  # Trainer flag\n",
    "    strategy: str = \"auto\"  # Trainer flag\n",
    "    precision: Optional[str] = \"bf16-mixed\"  # Trainer flag\n",
    "    max_epochs: int = 10  # Trainer flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and dataset\n",
    "model_name = ModuleConfig.model_name\n",
    "max_length = ModuleConfig.max_length\n",
    "lr = ModuleConfig.learning_rate\n",
    "dataset_name = DataModuleConfig.dataset_name\n",
    "batch_size = DataModuleConfig.batch_size\n",
    "\n",
    "# paths\n",
    "cache_dir = Config.cache_dir\n",
    "log_dir = Config.log_dir\n",
    "ckpt_dir = Config.ckpt_dir\n",
    "prof_dir = Config.prof_dir\n",
    "perf_dir = Config.perf_dir\n",
    "# creates dirs to avoid failure if empty dir has been deleted\n",
    "create_dirs([cache_dir, log_dir, ckpt_dir, prof_dir, perf_dir])\n",
    "\n",
    "# set matmul precision\n",
    "# see https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_datamodule = AutoTokenizerDataModule(\n",
    "    model_name=model_name,\n",
    "    dataset_name=dataset_name,\n",
    "    cache_dir=cache_dir,\n",
    "    batch_size=batch_size,\n",
    "    max_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_datamodule.clear_custom_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_datamodule.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_datamodule.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = CustomModel(learning_rate=lr, learning_rate_bert=lr, attention_probs_dropout=0.1, classifier_dropout=0, lr_gamma=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=ckpt_dir,\n",
    "        monitor=\"val_f1\",\n",
    "        filename=\"model\",\n",
    "        save_top_k=3,\n",
    "        mode=\"max\",\n",
    "        save_weights_only=True,\n",
    "    ),\n",
    "    LearningRateMonitor(logging_interval='step'),\n",
    "\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvlogger = CSVLogger(\n",
    "    save_dir=log_dir,\n",
    "    name=\"csv-logs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    strategy=\"auto\",\n",
    "    precision=TrainerConfig.precision,\n",
    "    max_epochs=8,\n",
    "    logger=[csvlogger, CometLogger(api_key=\"YOUR_COMET_API_KEY\"), DVCLiveLogger(save_dvc_exp=True)],\n",
    "    #logger=[csvlogger],\n",
    "    callbacks=callbacks,\n",
    "    log_every_n_steps=50,\n",
    "    #strategy=ray.train.lightning.RayDDPStrategy(find_unused_parameters=True),\n",
    "    #plugins=[ray.train.lightning.RayLightningEnvironment()],\n",
    "    #callbacks=[ray.train.lightning.RayTrainReportCallback(), LearningRateMonitor(logging_interval='epoch')],\n",
    "    # [1a] Optionally, disable the default checkpointing behavior\n",
    "    # in favor of the `RayTrainReportCallback` above.\n",
    "    #enable_checkpointing=False,\n",
    ")\n",
    "hyperparameters = dict(lr=lr)\n",
    "lit_trainer.logger.log_hyperparams(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_trainer.fit(model=lit_model, datamodule=lit_datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tw-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
